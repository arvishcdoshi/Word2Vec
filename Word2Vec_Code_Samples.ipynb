{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8cLEMTvKNBBmUnLzSe6h6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvishcdoshi/Word2Vec/blob/main/Word2Vec_Code_Samples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow00dxiF41Ow",
        "outputId": "ae036cad-63e1-48e0-dbf3-20390e483fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download adarshsng/googlenewsvectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO9jwucLCWir",
        "outputId": "1624c16a-11c3-4cc8-c84c-126241d1fbda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab0bd531"
      },
      "source": [
        "Please replace `YOUR_KAGGLE_USERNAME` and `YOUR_KAGGLE_KEY` with your actual Kaggle username and API key. Your Kaggle API key is a long alphanumeric string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71779ca2",
        "outputId": "60af412b-7af5-4996-9196-6eb3330e4894"
      },
      "source": [
        "#@title Enter your Kaggle credentials\n",
        "KAGGLE_USERNAME = \"arvishdoshi\" #@param {type:\"string\"}\n",
        "KAGGLE_KEY = \"KGAT_0748b4e4bf2f41fb2e191c0a3548d36c\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
        "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
        "\n",
        "print(\"Kaggle environment variables set. You can now try downloading datasets.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle environment variables set. You can now try downloading datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hc9f2kuoDcBL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "390c0d82"
      },
      "source": [
        "After running the above cell with your credentials, you can retry downloading the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "115e8c13"
      },
      "source": [
        "### Steps to set up Kaggle API credentials:\n",
        "\n",
        "1.  **Generate Kaggle API Token:**\n",
        "    *   Go to your Kaggle account page (www.kaggle.com/your_username/account).\n",
        "    *   Scroll down to the \"API\" section.\n",
        "    *   Click on \"Create New API Token\". This will download a file named `kaggle.json`.\n",
        "\n",
        "2.  **Upload `kaggle.json` to Colab:**\n",
        "    *   In the Colab file browser (the folder icon on the left sidebar), create a new directory named `.kaggle` (if it doesn't exist already).\n",
        "    *   Upload the `kaggle.json` file you just downloaded into the `.kaggle` directory.\n",
        "\n",
        "3.  **Configure Environment Variables:**\n",
        "    *   Run the following Python code to set the necessary environment variables. This code will move the `kaggle.json` file to the correct location and set the appropriate permissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17209a01",
        "outputId": "e0eed7ec-b32d-486b-bbf9-465f9d752c51"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist\n",
        "if not os.path.exists('/root/.kaggle'):\n",
        "    os.makedirs('/root/.kaggle')\n",
        "\n",
        "# Move the kaggle.json file to the correct directory\n",
        "# Assuming you uploaded kaggle.json to the Colab current working directory\n",
        "# If you uploaded it to /content/.kaggle, adjust the path accordingly\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "\n",
        "# Set permissions for the kaggle.json file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "print('Kaggle API credentials configured successfully!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Kaggle API credentials configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d842745"
      },
      "source": [
        "After running the above steps, you can try downloading the dataset again."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we execute the below command in Google Colab, you are instructing the Kaggle CLI to download the GoogleNews Word2Vec pre-trained embeddings from Kaggle.\n",
        "\n",
        "Although Kaggle lists it under “datasets,” this download actually contains a pre-trained model, not a traditional dataset. The file you get includes the GoogleNews-vectors-negative300.bin model, which is a binary Word2Vec embedding file originally trained by Google on the Google News corpus (about 100 billion words).\n",
        "\n",
        "***✔ What this model contains***\n",
        "\n",
        "Around 3 million English words and phrases\n",
        "\n",
        "Each represented as a 300-dimensional vector\n",
        "\n",
        "These embeddings capture semantic relationships (e.g., king - man + woman ≈ queen)\n",
        "\n",
        "***✔ Why it appears as a “dataset”***\n",
        "\n",
        "Kaggle hosts files in “datasets” sections even if the content is actually a model. So although the content is a pre-trained Word2Vec model, the Kaggle CLI treats it like any downloadable dataset.\n",
        "\n",
        "***✔ What the download command does***\n",
        "\n",
        "Downloads a zip file containing the Word2Vec model\n",
        "\n",
        "Saves it into your current working directory in Colab\n",
        "\n",
        "You can then unzip it and load it using libraries such as gensim"
      ],
      "metadata": {
        "id": "vpdr4BPYEMM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download adarshsng/googlenewsvectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQsigBOdDeEW",
        "outputId": "aa80ed9b-ff7d-449e-bef0-659397b68836"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adarshsng/googlenewsvectors\n",
            "License(s): unknown\n",
            "Downloading googlenewsvectors.zip to /content\n",
            "100% 1.63G/1.64G [00:16<00:00, 240MB/s]\n",
            "100% 1.64G/1.64G [00:16<00:00, 105MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"googlenewsvectors.zip\"\n",
        "extract_path = \"./\"  # Directory where it will be extracted\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Unzipping completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0kmSkosEcUz",
        "outputId": "2a5b78f6-68a4-42d6-b01e-9761e252d114"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# Load the pretrained Word2Vec model (in binary format)\n",
        "model_path = 'GoogleNews-vectors-negative300.bin'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "# Check if the model is loaded properly\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Example of finding the most similar words to a given word\n",
        "word = \"king\"\n",
        "print(f\"\\nMost similar words to '{word}':\")\n",
        "similar_words = model.most_similar(word, topn=10)\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity}\")\n",
        "\n",
        "# Example of word analogy: \"king is to queen as man is to woman\"\n",
        "print(\"\\nWord analogy: king - man + woman = ?\")\n",
        "result = model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\n",
        "print(f\"Answer: {result[0][0]} with similarity {result[0][1]}\")\n",
        "\n",
        "# Example of finding word similarity\n",
        "word1, word2 = \"king\", \"queen\"\n",
        "similarity = model.similarity(word1, word2)\n",
        "print(f\"\\nCosine similarity between '{word1}' and '{word2}': {similarity}\")\n",
        "\n",
        "# Example of finding the vector representation of a word\n",
        "word_vector = model['king']\n",
        "\n",
        "print(f\"\\nVector representation of the word 'king':\\n{word_vector}\")\n",
        "\n",
        "# Example of finding words that don't match in a set of words (odd one out)\n",
        "odd_one_out = model.doesnt_match([\"man\", \"woman\", \"king\", \"apple\"])\n",
        "print(f\"\\nOdd one out in the set ['man', 'woman', 'king', 'apple']: {odd_one_out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YXi-rTiEuYy",
        "outputId": "03bb02c7-aab3-429a-e89f-4e3f803aa0ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "Most similar words to 'king':\n",
            "kings: 0.7138045430183411\n",
            "queen: 0.6510956883430481\n",
            "monarch: 0.6413194537162781\n",
            "crown_prince: 0.6204220056533813\n",
            "prince: 0.6159993410110474\n",
            "sultan: 0.5864824056625366\n",
            "ruler: 0.5797567367553711\n",
            "princes: 0.5646552443504333\n",
            "Prince_Paras: 0.5432944297790527\n",
            "throne: 0.5422105193138123\n",
            "\n",
            "Word analogy: king - man + woman = ?\n",
            "Answer: queen with similarity 0.7118193507194519\n",
            "\n",
            "Cosine similarity between 'king' and 'queen': 0.6510956287384033\n",
            "\n",
            "Vector representation of the word 'king':\n",
            "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
            " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
            "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
            " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
            "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
            "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
            "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
            "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
            "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
            "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
            "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
            "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
            " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
            " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
            " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
            "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
            "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
            " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
            " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
            " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
            "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
            "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
            "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
            " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
            " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
            "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
            " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
            "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
            " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
            " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
            "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
            " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
            " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
            "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
            " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
            "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
            "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
            " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
            " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
            " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
            " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
            " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
            "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
            "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
            "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
            " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
            "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
            "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
            " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
            " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
            " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
            "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
            "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
            " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
            "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
            " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
            "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
            "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
            " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
            "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
            "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
            "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
            "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
            "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
            " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
            "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
            " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
            "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
            "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
            " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
            "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
            "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
            "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
            " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
            " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
            "\n",
            "Odd one out in the set ['man', 'woman', 'king', 'apple']: apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's use this pretrained Word2Vec model in the keras and tensorflow"
        "# We'll be using Word2Vec pre-trained embedding"
      ],
      "metadata": {
        "id": "N4LBZfEv9ofZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D\n",
        "import gensim\n",
        "\n",
        "# Load the pretrained Word2Vec model (in binary format)\n",
        "model_path = 'GoogleNews-vectors-negative300.bin'\n",
        "model_word2vec = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n",
        "embedding_dim = 300  # Google Word2Vec model has 300 dimensions\n",
        "vocab_size = len(model_word2vec.key_to_index)  # Total number of words in vocabulary\n",
        "\n",
        "# Initialize embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Create a mapping from words to their corresponding vectors\n",
        "word_index = {}  # This will store {word: index} mapping\n",
        "\n",
        "for i, word in enumerate(model_word2vec.key_to_index.keys()):\n",
        "    embedding_matrix[i] = model_word2vec[word]\n",
        "    word_index[word] = i  # Assign index to each word\n",
        "\n",
        "print(\"Embedding matrix created!\")\n",
        "\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=vocab_size,    # Size of vocabulary\n",
        "    output_dim=embedding_dim,  # Word vector dimensions (300)\n",
        "    weights=[embedding_matrix],  # Load pretrained Word2Vec weights\n",
        "    input_length=20,  # Length of input sequences (adjust as needed)\n",
        "    trainable=False  # Set to False to freeze the pretrained embeddings\n",
        ")\n",
        "\n",
        "print(\"Embedding layer created!\")\n",
        "\n",
        "\n",
        "# Define a simple model\n",
        "model = Sequential([\n",
        "    embedding_layer,  # Pretrained Word2Vec embeddings\n",
        "    GlobalAveragePooling1D(),  # Averages word vectors\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "summary = model.summary()\n",
        "print('Summary - ', summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dMTqiJf9vsj",
        "outputId": "122e0667-64ca-4ad3-8cbd-2048a51629d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}
